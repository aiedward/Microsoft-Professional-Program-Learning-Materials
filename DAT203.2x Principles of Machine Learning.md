# DAT203.2x: Principles of Machine Learning

## Module 1: Classification 
### Part I: Intro + Loss Function + Ockham's Razor
![classification loss function + Ockham's razor](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/Classification_loss_func.jpg)

### Part II: Logistic Regression
![clssification logistic regression](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/Classification_logistic_regression.jpg)

```python
# python code for logistic regreesion
from sklearn import linear_model
import numpy as np
df2 = df.get_dummies() # convert to categorical
X = df2[['a','b']].as_matrix() # numpy array of features
Y = df2.label.as_matrix().ravel() # 1-D label array

lg = linear_model.LogisticRegression(arguments)
logr = log.fit(X,Y)
score = logr.predict_log_proba(X)
```

### Part III: Evaluation Metrics

#### Checklist
* Confusion Matrix
* FP, FN, FP, FN
* Misclassification Error
* TPR(Recall), TNR(Specificity), FPR
* Precision
* F1 score

![Classifier Evaluation Metrics](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/Classification_Classifier_Evaluation_Metrics.jpg)


### Part IV: ROC Curves

#### Facts on ROC:
* The steeper the better
* Two ways of constructing ROC


![ROC curves](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/Classification_ROC1.jpg)
<hr>

## Module 2: Regression + Leverage Points
![regression](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/regression.jpg)

<hr>
## Module 3: Improving Machine Learning Models

### Outline
![improveModel_Outline](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/improveModel_outline.png)
### Feature Selection
![improveModel_feature_selection](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/improveModel_feature%20selection.jpg)
#### Notes: Use **"Permutation-based feature importance"** in Azure ML to find the ranking of feature importance.

### Regularization
![improveModel_regularization](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/improveModel_regularization.jpg)
#### Notes: L2 regularization automatically does feature selection, and is inclined to produce a sparse model.
### Scaling
![improveModel_scaling](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/improveModel_scaling.jpg)

### Sweeping Parameters
#### Notes: Use **"Tune Model Hyperparameters"** in Azure ML to choose the optimal hyperparameters.

### Cross Validation
![cross validation](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/improveModel_cross_validation.jpg)
#### Notes: Use **"Cross Validate Model"** in Azure ML to choose the optimal hyperparameters.
#### Notes: the input of the validate model is from the non-spliting data for training and testing.
Cross validation is used for inspecting whether the particular model is generalized well, whereas Nested cross validation is for tuning parameters.
![Nested cross validation](https://github.com/yang0339/Microsoft-Professional-Program-Learning-Materials/blob/master/DAT203.2x%20Principles%20of%20Machine%20Learning/improveModel_nested_cross_validation.jpg)


<hr>
## Module 4: Tree and Ensemble Methods

<hr>
## Module 5: Optimization

<hr>
## Module 6: Clustering and Recommenders

<hr>
### [Forked Course File on Github](https://github.com/yang0339/Principles-Of-Machine-Learning)
